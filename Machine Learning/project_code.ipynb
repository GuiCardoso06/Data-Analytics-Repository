{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"RTA Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Seleção de Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = ['Age_band_of_driver', 'Number_of_vehicles_involved', 'Number_of_casualties', 'Driving_experience', 'Type_of_collision', 'Type_of_vehicle', 'Area_accident_occured']\n",
    "\n",
    "#features = ['Age_band_of_driver', 'Number_of_vehicles_involved', 'Number_of_casualties', 'Driving_experience', 'Type_of_collision']\n",
    "\n",
    "features = ['Age_band_of_driver', 'Number_of_vehicles_involved', 'Number_of_casualties', 'Driving_experience']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tratamento do DataSet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformação de Valores Categóricos em Valores Numéricos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converter 'Driving_experience' para valores numéricos\n",
    "experiencia = {\n",
    "    'Less than 1yr': 1,\n",
    "    '1-2yr': 2,\n",
    "    '2-5yr': 3,\n",
    "    '5-10yr': 4,\n",
    "    'Above 10yr': 5\n",
    "}\n",
    "\n",
    "df['Driving_experience'] = df['Driving_experience'].map(experiencia)\n",
    "\n",
    "#Converter 'Age_band_of_driver' para valores numéricos \n",
    "idade = {\n",
    "    'Under 18': 1,\n",
    "    '18-30': 2,\n",
    "    '31-50': 3,\n",
    "    'Over 51': 4,\n",
    "    'Unknown': 5\n",
    "}\n",
    "\n",
    "df['Age_band_of_driver'] = df['Age_band_of_driver'].map(idade)\n",
    "\n",
    "#Converter 'Type_of_vehicle' para valores numéricos\n",
    "veiculo = {\n",
    "    'Automobile': 1,\n",
    "    'Public (> 45 seats)': 2,\n",
    "    'Lorry (41?100Q)': 3,\n",
    "    'Vehicle with vehicle collision': 4,\n",
    "    'Public (13?45 seats)': 5,\n",
    "    'Lorry (11?40Q)': 6,\n",
    "    'Long lorry': 7,\n",
    "    'Public (12 seats)': 8,\n",
    "    'Taxi': 9,\n",
    "    'Pick up upto 10Q': 10,\n",
    "    'Stationwagen': 11,\n",
    "    'Ridden horse': 12,\n",
    "    'Other': 13,\n",
    "    'Bajaj': 14,\n",
    "    'Turbo': 15,\n",
    "    'Motorcycle': 16,\n",
    "    'Special vehicle': 17,\n",
    "    'Bicycle': 18\n",
    "}\n",
    "\n",
    "df['Type_of_vehicle'] = df['Type_of_vehicle'].map(veiculo)\n",
    "\n",
    "#Converter 'Type_of_collision' para valores numéricos\n",
    "colisao = {\n",
    "    'Collision with roadside-parked vehicles': 1,\n",
    "    'Vehicle with vehicle collision': 2,\n",
    "    'Collision with roadside objects': 3,\n",
    "    'Collision with animals': 4,\n",
    "    'Other': 5,\n",
    "    'Rollover': 6,\n",
    "    'Fall from vehicles': 7,\n",
    "    'Collision with pedestrians': 8,\n",
    "    'With Train': 9,\n",
    "    'Unknown': 10\n",
    "}\n",
    "\n",
    "df['Type_of_collision'] = df['Type_of_collision'].map(colisao)\n",
    "\n",
    "#Converter 'Area_accident_occured' para valores numéricos\n",
    "area = {\n",
    "    'Residential areas': 1,\n",
    "    'Office areas': 2,\n",
    "    'Recreational areas': 3,\n",
    "    'Industrial areas': 4,\n",
    "    'Other': 5,\n",
    "    'Church areas': 6,\n",
    "    'Market areas': 7,\n",
    "    'Unknown': 8,\n",
    "    'Rural village areas': 9,\n",
    "    'Outside rural areas': 10,\n",
    "    'Hospital areas': 11,\n",
    "    'School areas': 12,\n",
    "    'Rural village areasOffice areas': 13  \n",
    "}\n",
    "\n",
    "df['Area_accident_occured'] = df['Area_accident_occured'].map(area)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valores em Falta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()\n",
    "\n",
    "#Substituir valores em falta pela mediana\n",
    "df['Driving_experience'].fillna(df['Driving_experience'].median(), inplace=True)\n",
    "\n",
    "#Substituir valores em falta pela moda\n",
    "mode_area_accident_occured = df['Area_accident_occured'].mode()[0]\n",
    "df['Area_accident_occured'].fillna(mode_area_accident_occured, inplace=True)\n",
    "\n",
    "mode_type_of_collision = df['Type_of_collision'].mode()[0]\n",
    "df['Type_of_collision'].fillna(mode_type_of_collision, inplace=True)\n",
    "\n",
    "mode_type_of_vehicle = df['Type_of_vehicle'].mode()[0]\n",
    "df['Type_of_vehicle'].fillna(mode_type_of_collision, inplace=True)\n",
    "\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PARTE 1 - DADOS NÃO BALANCEADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualização dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "severity_counts = df['Accident_severity'].value_counts()\n",
    "print(severity_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "severity_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribuição da Gravidade dos Acidentes')\n",
    "plt.xlabel('Gravidade do Acidente')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=0)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Juntar as duas Injurys\n",
    "df['Accident_severity2'] = df['Accident_severity'].replace({'Serious Injury': 'Serious/Fatal Injury', 'Fatal Injury': 'Serious/Fatal Injury', 'Fatal injury': 'Serious/Fatal Injury'})\n",
    "#print(df['Accident_severity2'].unique())\n",
    "\n",
    "severity_counts2 = df['Accident_severity2'].value_counts()\n",
    "print(severity_counts2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "severity_counts2.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribuição da Gravidade dos Acidentes')\n",
    "plt.xlabel('Gravidade do Acidente')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=0)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Padronização e Divisão de Dados em Teste/Treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-------------------------------------------------------3 Lesões-------------------------------------------------------\n",
    "X = df[features]\n",
    "y = df['Accident_severity']  \n",
    "\n",
    "#Divisão dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Padronização dos dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#-------------------------------------------------------2 Lesões-------------------------------------------------------\n",
    "\n",
    "X_2 = df[features]\n",
    "y_2 = df['Accident_severity2']  \n",
    "\n",
    "#Divisão dos dados\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
    "\n",
    "#Padronização dos dados\n",
    "scaler2 = StandardScaler()\n",
    "X_train_scaled2 = scaler2.fit_transform(X_train2)\n",
    "X_test_scaled2 = scaler2.transform(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelo SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "modelo_svm = SVC(random_state=42)\n",
    "\n",
    "parametros = {\n",
    "    'kernel': ['linear', 'rbf'],  #dois kernels\n",
    "    'C': reciprocal(1, 1000),  #distribuição log-uniforme\n",
    "    'gamma': expon(scale=1.0)  #distribuição exponencial\n",
    "}\n",
    "\n",
    "#-------------------------------------------------------3 Lesões-------------------------------------------------------\n",
    "\n",
    "search = RandomizedSearchCV(modelo_svm, parametros, n_iter=10, cv=3, scoring='f1_weighted', random_state=42, n_jobs=-1)\n",
    "\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Melhores parâmetros e melhor pontuação\n",
    "print(f\"Melhores parâmetros: {search.best_params_}\")\n",
    "print(f\"Melhor pontuação F1 ponderada: {search.best_score_ * 100:.2f}%\")\n",
    "\n",
    "#Avaliação\n",
    "best_model = search.best_estimator_\n",
    "y_pred_SVM = best_model.predict(X_test_scaled)\n",
    "print(\"Modelo SVM para 3 Lesões:\")\n",
    "print(classification_report(y_test, y_pred_SVM, zero_division=1))\n",
    "\n",
    "#-------------------------------------------------------2 Lesões-------------------------------------------------------\n",
    "\n",
    "search2 = RandomizedSearchCV(modelo_svm, parametros, n_iter=10, cv=3, scoring='f1_weighted', random_state=42, n_jobs=-1)\n",
    "\n",
    "search2.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "#Melhores parâmetros e melhor pontuação\n",
    "print(f\"Melhores parâmetros: {search2.best_params_}\")\n",
    "print(f\"Melhor pontuação F1 ponderada: {search2.best_score_ * 100:.2f}%\")\n",
    "\n",
    "#Avaliação\n",
    "best_model = search2.best_estimator_\n",
    "y_pred_SVM = best_model.predict(X_test_scaled2)\n",
    "print(\"Modelo SVM para 2 Lesões:\")\n",
    "print(classification_report(y_test2, y_pred_SVM, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes Neurais - Modelo Automático**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo Automático\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=74)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred_NN = mlp.predict(X_test_scaled)\n",
    "\n",
    "print(\"Modelo Redes Neurais para 3 Lesões:\")\n",
    "print(classification_report(y_test, y_pred_NN, zero_division=1)) #3 Lesões\n",
    "\n",
    "#MLPClassifier\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=74)\n",
    "mlp2.fit(X_train_scaled2, y_train2)\n",
    "y_pred_NN2 = mlp2.predict(X_test_scaled2)\n",
    "\n",
    "print(\"Modelo Redes Neurais para 2 Lesões:\")\n",
    "print(classification_report(y_test2, y_pred_NN2, zero_division=1)) #2 Lesões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes Neurais - Modelo Manual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo Manual\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Função de ativação sigmoid e sua derivada\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#-------------------------------------------------------3 Lesões-------------------------------------------------------\n",
    "\n",
    "#Codificação de classes (Valores Categóricos para Numéricos)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "#Converter y_train_encoded para one-hot (Saída da Rede Neural)\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "y_train_onehot = np.eye(num_classes)[y_train_encoded]\n",
    "\n",
    "#Inicialização dos pesos\n",
    "np.random.seed(42)\n",
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 10  \n",
    "output_size = num_classes\n",
    "\n",
    "weights1 = np.random.rand(input_size, hidden_size)\n",
    "weights2 = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "#treino\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #Forward propagation\n",
    "    hidden_layer_input = np.dot(X_train_scaled, weights1)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    final_output = sigmoid(np.dot(hidden_layer_output, weights2))\n",
    "    \n",
    "    #Backpropagation\n",
    "    error = y_train_onehot - final_output\n",
    "    d_predicted_output = error * sigmoid_derivative(final_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(weights2.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    #Atualização dos pesos\n",
    "    weights2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    weights1 += X_train_scaled.T.dot(d_hidden_layer) * learning_rate\n",
    "\n",
    "hidden_layer_input_test = np.dot(X_test_scaled, weights1)\n",
    "hidden_layer_output_test = sigmoid(hidden_layer_input_test)\n",
    "final_output_test = sigmoid(np.dot(hidden_layer_output_test, weights2))\n",
    "y_pred_classes = np.argmax(final_output_test, axis=1)\n",
    "\n",
    "#Converter previsões numéricas para rótulos categóricos\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "print(classification_report(y_test_encoded, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "#-------------------------------------------------------2 Lesões-------------------------------------------------------\n",
    "\n",
    "# Codificação de classes\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train2)\n",
    "y_test_encoded = label_encoder.transform(y_test2)\n",
    "\n",
    "#Inicialização dos pesos\n",
    "np.random.seed(42)\n",
    "input_size = X_train_scaled2.shape[1]\n",
    "hidden_size = 10  #valor escolhido\n",
    "output_size = 1  # Um neurônio de saída para classificação binária\n",
    "\n",
    "weights1 = np.random.rand(input_size, hidden_size)\n",
    "weights2 = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "#treino\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #Forward propagation\n",
    "    hidden_layer_input = np.dot(X_train_scaled2, weights1)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    final_output = sigmoid(np.dot(hidden_layer_output, weights2))\n",
    "    \n",
    "    #Backpropagation\n",
    "    error = y_train_encoded.reshape(-1, 1) - final_output\n",
    "    d_predicted_output = error * sigmoid_derivative(final_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(weights2.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    #Atualização dos pesos\n",
    "    weights2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    weights1 += X_train_scaled2.T.dot(d_hidden_layer) * learning_rate\n",
    "\n",
    "hidden_layer_input_test = np.dot(X_test_scaled2, weights1)\n",
    "hidden_layer_output_test = sigmoid(hidden_layer_input_test)\n",
    "final_output_test = sigmoid(np.dot(hidden_layer_output_test, weights2))\n",
    "y_pred_classes = (final_output_test > 0.5).astype(int).flatten()\n",
    "\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "print(classification_report(y_test_encoded, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#-------------------------------------------------------3 Lesões-------------------------------------------------------\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "random_forest.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_RF = random_forest.predict(X_test_scaled)\n",
    "\n",
    "print(\"Modelo RandomForest para 3 Lesões:\")\n",
    "print(classification_report(y_test, y_pred_RF, zero_division=1))\n",
    "\n",
    "#-------------------------------------------------------2 Lesões-------------------------------------------------------\n",
    "\n",
    "random_forest2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "random_forest2.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "y_pred_RF2 = random_forest2.predict(X_test_scaled2)\n",
    "\n",
    "print(\"Modelo RandomForest para 2 Lesões:\")\n",
    "print(classification_report(y_test2, y_pred_RF2, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PARTE 2 - DADOS BALANCEADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Balanceamento dos Dados, Padronização e Divisão de Dados em Teste/Treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "#-------------------------------------------------------3 Lesões-------------------------------------------------------\n",
    "\n",
    "X = df[features]\n",
    "y = df['Accident_severity']  \n",
    "\n",
    "#Divisão dos dados para 3 Lesões\n",
    "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#-------------------------------------------------------2 Lesões-------------------------------------------------------\n",
    "\n",
    "X_2 = df[features]\n",
    "y_2 = df['Accident_severity2']  \n",
    "\n",
    "#Divisão dos dados para 2 Lesões\n",
    "X_train_balanced2, X_test_balanced2, y_train_balanced2, y_test_balanced2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
    "\n",
    "#--------------------------------------------------Balanceamento dos dados--------------------------------------------------\n",
    "\n",
    "#Balanceamento dos dados para 3 Lesões\n",
    "train_combined = pd.concat([X_train_balanced, y_train_balanced.reset_index(drop=True)], axis=1)\n",
    "\n",
    "slight_injury = train_combined[train_combined['Accident_severity'] == 'Slight Injury']\n",
    "fatal_injury = train_combined[train_combined['Accident_severity'] == 'Fatal injury']\n",
    "serious_injury = train_combined[train_combined['Accident_severity'] == 'Serious Injury']\n",
    "\n",
    "serious_injury_upsampled = resample(serious_injury, replace=True, n_samples=len(slight_injury), random_state=42)\n",
    "fatal_injury_upsampled = resample(fatal_injury, replace=True, n_samples=len(slight_injury), random_state=42)\n",
    "\n",
    "balanced_data = pd.concat([slight_injury, serious_injury_upsampled, fatal_injury_upsampled])\n",
    "\n",
    "X_train_balanced = balanced_data.drop('Accident_severity', axis=1)\n",
    "y_train_balanced = balanced_data['Accident_severity']\n",
    "\n",
    "#Balanceamento dos dados para 2 Lesões\n",
    "train_combined2 = pd.concat([X_train_balanced2, y_train_balanced2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "slight_injury2 = train_combined2[train_combined2['Accident_severity2'] == 'Slight Injury']\n",
    "serious_fatal_injury2 = train_combined2[train_combined2['Accident_severity2'] == 'Serious/Fatal Injury']\n",
    "\n",
    "serious_fatal_injury_upsampled2 = resample(serious_fatal_injury2, replace=True, n_samples=len(slight_injury2), random_state=42)\n",
    "\n",
    "balanced_data2 = pd.concat([slight_injury2, serious_fatal_injury_upsampled2])\n",
    "\n",
    "X_train_balanced2 = balanced_data2.drop('Accident_severity2', axis=1)\n",
    "y_train_balanced2 = balanced_data2['Accident_severity2']\n",
    "\n",
    "#-----------------------------------------------------Padronização dos dados--------------------------------------------------\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Padronização dos dados para 3 Lesões\n",
    "X_train_balanced_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_balanced_scaled = scaler.transform(X_test_balanced)\n",
    "\n",
    "#Padronização dos dados para 2 Lesões\n",
    "X_train_balanced_scaled2 = scaler.fit_transform(X_train_balanced2)\n",
    "X_test_balanced_scaled2 = scaler.transform(X_test_balanced2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualização dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_counts2 = y_train_balanced2.value_counts()\n",
    "print(balanced_counts2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "balanced_counts2.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribuição da Gravidade dos Acidentes (Após Balanceamento)')\n",
    "plt.xlabel('Gravidade do Acidente')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=0)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "balanced_counts = y_train_balanced.value_counts()\n",
    "print(balanced_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "balanced_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribuição da Gravidade dos Acidentes (Após Balanceamento)')\n",
    "plt.xlabel('Gravidade do Acidente')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=0)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelo SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "#preencher valores ausentes\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_balanced_scaled_imputed = imputer.fit_transform(X_train_balanced_scaled)\n",
    "X_test_balanced_scaled_imputed = imputer.transform(X_test_balanced_scaled)\n",
    "\n",
    "X_train_balanced_scaled2_imputed = imputer.fit_transform(X_train_balanced_scaled2)\n",
    "X_test_balanced_scaled2_imputed = imputer.transform(X_test_balanced_scaled2)\n",
    "\n",
    "#Modelo SVM\n",
    "modelo_svm = SVC(random_state=42)\n",
    "\n",
    "# Parâmetros para pesquisa aleatória\n",
    "parametros = {\n",
    "    'kernel': ['linear', 'rbf'],  #Dois tipos de kernel\n",
    "    'C': reciprocal(1, 1000),      #Distribuição log-uniforme\n",
    "    'gamma': expon(scale=1.0)      #Distribuição exponencial\n",
    "}\n",
    "\n",
    "#-------------------------------------------------------3 Lesões-------------------------------------------------------\n",
    "\n",
    "search = RandomizedSearchCV(modelo_svm, parametros, n_iter=10, cv=3, scoring='f1_weighted', random_state=42, n_jobs=-1)\n",
    "search.fit(X_train_balanced_scaled_imputed, y_train_balanced)\n",
    "\n",
    "print(f\"Melhores parâmetros para 3 Lesões: {search.best_params_}\")\n",
    "print(f\"Melhor pontuação F1 ponderada para 3 Lesões: {search.best_score_ * 100:.2f}%\")\n",
    "\n",
    "y_pred_SVM = search.best_estimator_.predict(X_test_balanced_scaled_imputed)\n",
    "print(\"\\nModelo SVM para 3 Lesões:\")\n",
    "print(classification_report(y_test_balanced, y_pred_SVM, zero_division=1))\n",
    "\n",
    "#-------------------------------------------------------2 Lesões-------------------------------------------------------\n",
    "\n",
    "search2 = RandomizedSearchCV(modelo_svm, parametros, n_iter=10, cv=3, scoring='f1_weighted', random_state=42, n_jobs=-1)\n",
    "search2.fit(X_train_balanced_scaled2_imputed, y_train_balanced2)\n",
    "\n",
    "print(f\"\\nMelhores parâmetros para 2 Lesões: {search2.best_params_}\")\n",
    "print(f\"Melhor pontuação F1 ponderada para 2 Lesões: {search2.best_score_ * 100:.2f}%\")\n",
    "\n",
    "y_pred_SVM2 = search2.best_estimator_.predict(X_test_balanced_scaled2_imputed)\n",
    "print(\"\\nModelo SVM para 2 Lesões:\")\n",
    "print(classification_report(y_test_balanced2, y_pred_SVM2, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes Neurais - Modelo Automático**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mlp_balanced = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=74)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_balanced_scaled_imputed = imputer.fit_transform(X_train_balanced_scaled)\n",
    "X_test_balanced_scaled_imputed = imputer.transform(X_test_balanced_scaled)\n",
    "\n",
    "mlp_balanced.fit(X_train_balanced_scaled_imputed, y_train_balanced)\n",
    "y_pred_NN_balanced = mlp_balanced.predict(X_test_balanced_scaled_imputed)\n",
    "\n",
    "print(\"Modelo Redes Neurais para 3 Lesões (Dados Balanceados):\")\n",
    "print(classification_report(y_test_balanced, y_pred_NN_balanced, zero_division=1))#3 Lesões\n",
    "\n",
    "mlp_balanced2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=74)\n",
    "\n",
    "#substituir valores ausentes com a média\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_balanced_scaled2_imputed = imputer.fit_transform(X_train_balanced_scaled2)\n",
    "X_test_balanced_scaled2_imputed = imputer.transform(X_test_balanced_scaled2)\n",
    "\n",
    "mlp_balanced2.fit(X_train_balanced_scaled2_imputed, y_train_balanced2)\n",
    "y_pred_NN_balanced2 = mlp_balanced2.predict(X_test_balanced_scaled2_imputed)\n",
    "\n",
    "print(\"Modelo Redes Neurais para 2 Lesões (Dados Balanceados):\")\n",
    "print(classification_report(y_test_balanced2, y_pred_NN_balanced2, zero_division=1))#2 Lesões\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes Neurais - Modelo Manual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Função de ativação sigmoid e sua derivada\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#-------------------------------------------------------3 Lesões com dados balanceados-------------------------------------------------------\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_balanced)\n",
    "y_test_encoded = label_encoder.transform(y_test_balanced)\n",
    "\n",
    "#Converter y_train_encoded para one-hot\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "y_train_onehot = np.eye(num_classes)[y_train_encoded]\n",
    "\n",
    "np.random.seed(42)\n",
    "input_size = X_train_balanced_scaled.shape[1]\n",
    "hidden_size = 10  \n",
    "output_size = num_classes\n",
    "\n",
    "weights1 = np.random.rand(input_size, hidden_size)\n",
    "weights2 = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden_layer_input = np.dot(X_train_balanced_scaled, weights1)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    final_output = sigmoid(np.dot(hidden_layer_output, weights2))\n",
    "    \n",
    "    error = y_train_onehot - final_output\n",
    "    d_predicted_output = error * sigmoid_derivative(final_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(weights2.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    weights2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    weights1 += X_train_balanced_scaled.T.dot(d_hidden_layer) * learning_rate\n",
    "\n",
    "#dados de teste\n",
    "hidden_layer_input_test = np.dot(X_test_balanced_scaled, weights1)\n",
    "hidden_layer_output_test = sigmoid(hidden_layer_input_test)\n",
    "final_output_test = sigmoid(np.dot(hidden_layer_output_test, weights2))\n",
    "y_pred_classes = np.argmax(final_output_test, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "print(classification_report(y_test_encoded, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "#-------------------------------------------------------2 Lesões com dados balanceados-------------------------------------------------------\n",
    "\n",
    "#codificação de classes\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_balanced2)\n",
    "y_test_encoded = label_encoder.transform(y_test_balanced2)\n",
    "\n",
    "np.random.seed(42)\n",
    "input_size = X_train_balanced_scaled2.shape[1]\n",
    "hidden_size = 10  #valor escolhido\n",
    "output_size = 1  #Um neurônio de saída para classificação binária\n",
    "\n",
    "weights1 = np.random.rand(input_size, hidden_size)\n",
    "weights2 = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "#treino\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #Forward propagation\n",
    "    hidden_layer_input = np.dot(X_train_balanced_scaled2, weights1)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    final_output = sigmoid(np.dot(hidden_layer_output, weights2))\n",
    "    \n",
    "    #Backpropagation\n",
    "    error = y_train_encoded.reshape(-1, 1) - final_output\n",
    "    d_predicted_output = error * sigmoid_derivative(final_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(weights2.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    #Atualização dos pesos\n",
    "    weights2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    weights1 += X_train_balanced_scaled2.T.dot(d_hidden_layer) * learning_rate\n",
    "\n",
    "#dados de teste\n",
    "hidden_layer_input_test = np.dot(X_test_balanced_scaled2, weights1)\n",
    "hidden_layer_output_test = sigmoid(hidden_layer_input_test)\n",
    "final_output_test = sigmoid(np.dot(hidden_layer_output_test, weights2))\n",
    "y_pred_classes = (final_output_test > 0.5).astype(int).flatten()\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "print(classification_report(y_test_encoded, y_pred_classes, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "#preencher valores ausentes nos dados de treinamento e teste\n",
    "X_train_balanced_scaled_imputed = imputer.fit_transform(X_train_balanced_scaled)\n",
    "X_test_balanced_scaled_imputed = imputer.transform(X_test_balanced_scaled)\n",
    "\n",
    "X_train_balanced_scaled2_imputed = imputer.fit_transform(X_train_balanced_scaled2)\n",
    "X_test_balanced_scaled2_imputed = imputer.transform(X_test_balanced_scaled2)\n",
    "\n",
    "#-------------------------------------------------------3 Lesões-------------------------------------------------------\n",
    "\n",
    "random_forest_balanced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_balanced.fit(X_train_balanced_scaled_imputed, y_train_balanced)\n",
    "\n",
    "y_pred_RF_balanced = random_forest_balanced.predict(X_test_balanced_scaled_imputed)\n",
    "\n",
    "print(\"Modelo RandomForest para 3 Lesões (Dados Balanceados):\")\n",
    "print(classification_report(y_test_balanced, y_pred_RF_balanced, zero_division=1))\n",
    "\n",
    "#-------------------------------------------------------2 Lesões-------------------------------------------------------\n",
    "\n",
    "random_forest_balanced2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_balanced2.fit(X_train_balanced_scaled2_imputed, y_train_balanced2)\n",
    "\n",
    "y_pred_RF_balanced2 = random_forest_balanced2.predict(X_test_balanced_scaled2_imputed)\n",
    "\n",
    "print(\"Modelo RandomForest para 2 Lesões (Dados Balanceados):\")\n",
    "print(classification_report(y_test_balanced2, y_pred_RF_balanced2, zero_division=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
