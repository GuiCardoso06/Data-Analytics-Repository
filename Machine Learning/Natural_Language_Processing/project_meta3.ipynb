{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "META 3\n",
    "PLN - PROCESSAMENTO DE LINGUAGEM NATURAL (LECD) 2023/2024\n",
    "NOME: Guilherme Cardoso / Francisco Rua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from googletrans import Translator\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\") #modelo do spacy em PT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARREGAR BANCO DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = pd.read_json(\"dialogosCoimbra.json\")\n",
    "\n",
    "hotels = pd.read_json(\"hotelsCoimbra_db.json\")\n",
    "\n",
    "attractions = pd.read_json(\"attractionsCoimbra_db.json\")\n",
    "\n",
    "trains = pd.read_json(\"trainsCoimbra_db.json\")\n",
    "\n",
    "restaurants = pd.read_json(\"restaurantsCoimbra_db.json\")\n",
    "\n",
    "#print(dialogues)\n",
    "#print(hotels)\n",
    "#print(attractions)\n",
    "#print(trains)\n",
    "#print(restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALISAR DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dialogues.info() #0 valores em falta\n",
    "#hotels.info() #0 valores em falta\n",
    "#attractions.info() #0 valores em falta\n",
    "#trains.info() #0 valores em falta\n",
    "#restaurants.info() #79 valores em falta (diferentes colunas)\n",
    "\n",
    "hotels_duplicates = hotels.duplicated(subset=['id', 'nameMultiWoz'])\n",
    "total_duplicates = hotels_duplicates.sum()\n",
    "#print(str(total_duplicates) + ' - Hotels') #0 valores duplicados\n",
    "\n",
    "attractions_duplicates = attractions.duplicated(subset=['id', 'nameMultiWoz'])\n",
    "total_duplicates = attractions_duplicates.sum()\n",
    "#print(str(total_duplicates) + ' - Attractions') #0 valores duplicados\n",
    "\n",
    "trains_duplicates = trains.duplicated(subset=['id', 'nameMultiWoz'])\n",
    "total_duplicates = trains_duplicates.sum()\n",
    "#print(str(total_duplicates) + ' - Trains') #0 valores duplicados\n",
    "\n",
    "restaurants_duplicates = restaurants.duplicated(subset=['id', 'nameMultiWoz'])\n",
    "total_duplicates = restaurants_duplicates.sum()\n",
    "#print(str(total_duplicates) + ' - Restaurants') #0 valores duplicados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBSTITUIÇÃO DOS VALORES EM FALTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants['area'].fillna('!', inplace=True)\n",
    "restaurants['introduction'].fillna('!', inplace=True)\n",
    "restaurants['phone'].fillna('!', inplace=True)\n",
    "restaurants['signature'].fillna('!', inplace=True)\n",
    "\n",
    "#restaurants.info() #0 valores em falta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRAÇÃO DE PERGUNTAS / RESPOSTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando list comprehensions para extrair perguntas e respostas\n",
    "questions = [sentence['utterance'] for _, d in dialogues.iterrows() for sentence in d['turns'] if sentence.get('speaker') == \"USER\"]\n",
    "answers = [sentence['utterance'] for _, d in dialogues.iterrows() for sentence in d['turns'] if sentence.get('speaker') == \"SYSTEM\"]\n",
    "\n",
    "#print(questions)\n",
    "# print(answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMALIZAÇÃO DO TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove espaços extras\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove caracteres especiais\n",
    "    return text\n",
    "\n",
    "questions = [normalize_text(q) for q in questions]\n",
    "answers = [normalize_text(a) for a in answers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANÁLISE BÁSICA DE PERGUNTAS E RESPOSTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Análise do comprimento das perguntas e respostas\n",
    "questions_lengths = [len(q.split()) for q in questions]\n",
    "answers_lengths = [len(a.split()) for a in answers]\n",
    "\n",
    "print(f\"Comprimento médio das perguntas: {pd.Series(questions_lengths).mean()}\")\n",
    "print(f\"Comprimento médio das respostas: {pd.Series(answers_lengths).mean()}\")\n",
    "\n",
    "# Frequência de palavras\n",
    "all_words = ' '.join(questions + answers)\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "# Visualização com WordCloud\n",
    "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIVIDIR DADOS PARA TREINO E TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_questions, test_questions, train_answers, test_answers = train_test_split(\n",
    "    questions, answers, test_size=0.2, random_state=42)  #20% dos dados são reservados para teste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZAÇÃO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Inicializa o tokenizer do BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def find_answer_positions(tokenizer, contexts, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for context, answer in zip(contexts, answers):\n",
    "        # Tokenize the context and the answer separately\n",
    "        context_enc = tokenizer.encode(context, add_special_tokens=True)\n",
    "        answer_enc = tokenizer.encode(answer, add_special_tokens=True)\n",
    "        \n",
    "        # Remove the CLS and SEP tokens from the answer encoding\n",
    "        answer_enc = answer_enc[1:-1]\n",
    "        \n",
    "        # Search for the answer encoding sequence within the context encoding\n",
    "        for index in range(len(context_enc) - len(answer_enc) + 1):\n",
    "            if context_enc[index:index+len(answer_enc)] == answer_enc:\n",
    "                # Found the answer start and end positions\n",
    "                start_positions.append(index)\n",
    "                end_positions.append(index + len(answer_enc) - 1)\n",
    "                break\n",
    "        else:\n",
    "            # Answer not found in the context, this is an issue\n",
    "            print(f\"Answer not found in the context: {context} / {answer}\")\n",
    "            # For now, we can set them to the index of the first token, but you'll need to handle this\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            \n",
    "    return start_positions, end_positions\n",
    "\n",
    "# Assuming train_contexts and train_answers are your training questions and answers lists\n",
    "train_start_positions, train_end_positions = find_answer_positions(tokenizer, train_questions, train_answers)\n",
    "test_start_positions, test_end_positions = find_answer_positions(tokenizer, test_questions, test_answers)\n",
    "\n",
    "# Convert the positions into tensors\n",
    "train_start_positions = torch.tensor(train_start_positions)\n",
    "train_end_positions = torch.tensor(train_end_positions)\n",
    "test_start_positions = torch.tensor(test_start_positions)\n",
    "test_end_positions = torch.tensor(test_end_positions)\n",
    "\n",
    "# Tokenize all the contexts (questions in this case) with padding to the maximum length\n",
    "train_encodings = tokenizer(train_questions, padding=True, truncation=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_questions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Create the TensorDatasets\n",
    "train_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'],\n",
    "    train_encodings['attention_mask'],\n",
    "    train_start_positions,\n",
    "    train_end_positions\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'],\n",
    "    test_encodings['attention_mask'],\n",
    "    test_start_positions,\n",
    "    test_end_positions\n",
    ")\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREINAR O MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, AdamW\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "# Carregar o modelo pré-treinado\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Mover o modelo para a GPU, se disponível\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Configurar o otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Definir o número de épocas para o treinamento\n",
    "num_epochs = 3\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(num_epochs):\n",
    "    # Colocar o modelo em modo de treinamento\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Mover o batch para o dispositivo apropriado\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'start_positions': batch[2],\n",
    "            'end_positions': batch[3]\n",
    "        }\n",
    "        \n",
    "        # Zerar os gradientes do modelo\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Realizar um passo de treinamento\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calcular a perda média sobre todos os batches\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Average training loss: {avg_train_loss}')\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model_path = \"./bert_for_qa.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVALIAÇÃO DO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "total_eval_loss = 0\n",
    "for batch in test_dataloader:\n",
    "    # Mover o batch para o dispositivo apropriado\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {\n",
    "        'input_ids': batch[0],\n",
    "        'attention_mask': batch[1],\n",
    "        'start_positions': batch[2],\n",
    "        'end_positions': batch[3]\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Realizar uma passagem de avaliação\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "# Calcular a perda média sobre todos os batches\n",
    "avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "print(f'Average evaluation loss: {avg_eval_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Certifique-se de ter os pacotes necessários\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extract_keywords(question):\n",
    "    stop_words = set(stopwords.words('english'))  # Use 'portuguese' para stop words em português\n",
    "    word_tokens = word_tokenize(question.lower())\n",
    "\n",
    "    keywords = [word for word in word_tokens if word not in stop_words and word.isalpha()]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keyword_index(dataframe):\n",
    "    keyword_index = {}\n",
    "    for coluna in dataframe.columns:\n",
    "        if dataframe[coluna].dtype == 'object':  # Verifica se a coluna é textual\n",
    "            for index, item in enumerate(dataframe[coluna]):\n",
    "                if isinstance(item, str):\n",
    "                    for word in item.lower().split():\n",
    "                        if word not in keyword_index:\n",
    "                            keyword_index[word] = []\n",
    "                        keyword_index[word].append(index)\n",
    "    return keyword_index\n",
    "\n",
    "# Cria índices de palavras-chave para cada dataframe\n",
    "hotels_index = create_keyword_index(hotels)\n",
    "attractions_index = create_keyword_index(attractions)\n",
    "trains_index = create_keyword_index(trains)\n",
    "restaurants_index = create_keyword_index(restaurants)\n",
    "\n",
    "def find_context_in_json(keywords, dataframes_indices):\n",
    "    # Verifica em qual índice as palavras-chave aparecem mais frequentemente\n",
    "    max_count = 0\n",
    "    relevant_context = None\n",
    "\n",
    "    for word in keywords:\n",
    "        for df_name, index in dataframes_indices.items():\n",
    "            if word in index:\n",
    "                count = len(index[word])\n",
    "                if count > max_count:\n",
    "                    max_count = count\n",
    "                    relevant_context = df_name\n",
    "\n",
    "    return relevant_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_text(df_name, keywords, dataframes):\n",
    "    \n",
    "    df = dataframes[df_name]\n",
    "\n",
    "    # Compila todos os textos que contêm as palavras-chave\n",
    "    context_texts = []\n",
    "    for keyword in keywords:\n",
    "        for coluna in df.columns:\n",
    "            if df[coluna].dtype == 'object':  # Verifica se a coluna é textual\n",
    "                matching_texts = df[df[coluna].str.contains(keyword, na=False, case=False)]\n",
    "                context_texts.extend(matching_texts[coluna].tolist())\n",
    "\n",
    "    context = \" \".join(set(context_texts))  # Usando set para remover duplicatas\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, model, tokenizer):\n",
    "    model.eval()\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    # Localiza o índice do token [SEP] que separa a pergunta do contexto\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # Obtém os tokens da resposta a partir do input_ids original\n",
    "    answer_tokens = input_ids[sep_index + 1:]\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_qa(model, tokenizer, dataframes_indices, dataframes):\n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"Digite a pergunta (ou 'sair' para encerrar): \")\n",
    "            if question.lower() == 'sair':\n",
    "                print(\"Saindo do sistema de QA.\")\n",
    "                break\n",
    "\n",
    "            keywords = extract_keywords(question)\n",
    "            context_df_name = find_context_in_json(keywords, dataframes_indices)\n",
    "\n",
    "            if context_df_name is not None:\n",
    "                context_text = get_context_text(context_df_name, keywords, dataframes)\n",
    "                answer = answer_question(question, context_text, model, tokenizer)\n",
    "                print(f\"Resposta: {answer}\\n\")\n",
    "            else:\n",
    "                print(\"Não foi possível encontrar um contexto relevante para a pergunta.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro: {e}\")\n",
    "            break\n",
    "\n",
    "dataframes_indices = {\n",
    "    'hotels': hotels_index,\n",
    "    'attractions': attractions_index,\n",
    "    'trains': trains_index,\n",
    "    'restaurants': restaurants_index\n",
    "}\n",
    "\n",
    "dataframes = {\n",
    "    'hotels': hotels,\n",
    "    'attractions': attractions,\n",
    "    'trains': trains,\n",
    "    'restaurants': restaurants\n",
    "}\n",
    "\n",
    "\n",
    "# Inicia a interface interativa\n",
    "interactive_qa(model, tokenizer, dataframes_indices, dataframes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
